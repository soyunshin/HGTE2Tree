## Heterogeneous Graphs using Graph Transformer Encoder 

This repository is the implementation of Integrating Heterogeneous Graphs using Graph Transformer Encoder for Solving Math Word Problems.


![Model Architecture5](https://user-images.githubusercontent.com/70592135/215336026-bbd50c2c-d090-405b-985e-1913960e9401.png)


In this work, we introduces a novel method that integrates structural information with training deep neural models to solve math word problems.
To provide various types of structural information in a uniform way, we propose a graph transformer encoder to integrate heterogeneous graphs of various input representations.
Experimental results show that our method produces competitive results compared to the baselines. 
Moreover, we discuss that integrating different types of textual characteristics may improve the quality of mathematical logic inference from natural language sentences.

<br>

## Requirements
- Ubuntu 18.04 LTS
- Python 3.6.9
- Pytorch 1.7.1
- transformers 3.3.1
- nltk 3.6.6
- gensim 3.8.0

<br>

## Acknowledge
Some of the codes are built upon [Graph-to-Tree](https://github.com/2003pro/Graph2Tree) and [SVAMP](https://github.com/arkilpatel/SVAMP).<br>
You are encouraged to submit issues and contribute pull requests.
